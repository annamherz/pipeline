{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write ligands seperately\n",
    "\n",
    "mols = Chem.SDMolSupplier(\n",
    "    \"/home/anna/Documents/benchmark/inputs/industrybenchmarks2024/p38/schroedinger22jacs_ligands.sdf\"\n",
    ")\n",
    "for mol in mols:\n",
    "    # print(mol.GetProp(\"_Name\"))\n",
    "    with Chem.SDWriter(\n",
    "        f'/home/anna/Documents/benchmark/inputs/industrybenchmarks2024/p38/ligands_schroedinger22jacs_intermediates/lig_{mol.GetProp(\"_Name\").replace(\"p38a_\",\"\").replace(\"_\",\"\")}.sdf'\n",
    "    ) as w:\n",
    "        w.write(mol)\n",
    "\n",
    "# rename\n",
    "name_dict = {}\n",
    "for prot in [\"syk\", \"hif2a\", \"cmet\"]:\n",
    "    jacs_file = f\"/home/anna/Documents/benchmark/inputs/{prot}/results_edges_5ns.csv\"\n",
    "    mols = Chem.SDMolSupplier(\n",
    "        f\"/home/anna/Documents/benchmark/inputs/{prot}/ligands.sdf\"\n",
    "    )\n",
    "    name_dict[prot] = {}\n",
    "    for mol, idx in zip(mols, range(1, len(mols) + 1, 1)):\n",
    "        name_dict[prot][mol.GetProp(\"_Name\")] = f\"lig_{idx}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# professa results\n",
    "\n",
    "# file = f\"/home/anna/Documents/benchmark/inputs/other_computed/professa/professa_{prot}_results.dat\"\n",
    "\n",
    "# df = pd.read_csv(file, delimiter=\",\")\n",
    "\n",
    "# perts_dict = {}\n",
    "# for index, row in df.iterrows():\n",
    "#     perts_dict[f\"{row['perturbation']}\"] = (float(row[\"ddG\"]), float(row[\"ddG_error\"]))\n",
    "# write_perts_file(\n",
    "#     perts_dict,\n",
    "#     # .csv\n",
    "#     file_path=f\"/home/anna/Documents/benchmark/inputs/{prot}/perts_file_professa\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting stats distributions\n",
    "results_dict = net_proc_dict\n",
    "\n",
    "# make best fit dict\n",
    "for protein in [\"tyk2\", \"mcl1\"]:  # , \"p38\"\n",
    "    print(protein)\n",
    "    best_fit_dict = {}\n",
    "\n",
    "    for network in [\"lomap\", \"rbfenn\"]:\n",
    "        for eng in ana_obj.engines:\n",
    "            mu, std, ci = results_dict[network][protein][eng]\n",
    "\n",
    "            try:\n",
    "                # Plot the PDF.\n",
    "                if stats == \"spearmans_coeff\":\n",
    "                    xmin, xmax = plt.xlim(left=-1, right=1)\n",
    "                elif stats == \"MAE\":\n",
    "                    xmin, xmax = plt.xlim(left=0, right=mu + 1)\n",
    "                x = np.linspace(xmin, xmax, 100)\n",
    "                y = norm.pdf(x, mu, std)\n",
    "\n",
    "                best_fit_dict[f\"{network}_{eng}\"] = ((x, y), mu, std)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    col_dict = {}\n",
    "\n",
    "    for key in best_fit_dict.keys():\n",
    "        hexadecimal_alphabets = \"0123456789ABCDEF\"\n",
    "        color = \"#\" + \"\".join([random.choice(hexadecimal_alphabets) for j in range(6)])\n",
    "        col_dict[key] = color\n",
    "\n",
    "    # plot the distributions\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    lines = []\n",
    "    mu_std_string = \"\"\n",
    "\n",
    "    for name in best_fit_dict.keys():\n",
    "        col = col_dict[name]\n",
    "        plt.plot(\n",
    "            best_fit_dict[name][0][0],\n",
    "            best_fit_dict[name][0][1],\n",
    "            \"k\",\n",
    "            linewidth=2,\n",
    "            color=col,\n",
    "        )\n",
    "        lines += plt.plot(0, 0, c=col, label=name)\n",
    "        mu_std_string += f\"\\n{name} : mu = {best_fit_dict[name][1]:.3f} , std = {best_fit_dict[name][2]:.3f}\"\n",
    "\n",
    "        ax.axvspan(\n",
    "            best_fit_dict[name][1] + best_fit_dict[name][2],\n",
    "            best_fit_dict[name][1] - best_fit_dict[name][2],\n",
    "            color=col,\n",
    "            alpha=0.2,\n",
    "        )\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    plt.legend(lines, labels, loc=\"upper right\")\n",
    "\n",
    "    plt.xlabel(\"Error\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"{protein}, {stats}\\n{mu_std_string}\")\n",
    "    plt.savefig(\n",
    "        f\"/backup/overall_analysis/{protein}_rbfenn_lomap_{stats}_distribution_from_bootstrapping.png\",\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RERUNS ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun analysis\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "# import libraries\n",
    "from pipeline.analysis import *\n",
    "from pipeline.utils import *\n",
    "from pipeline import *\n",
    "import logging\n",
    "from scipy.stats import sem as sem\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "print(BSS.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_comprehension(matrix):\n",
    "    return [item for row in matrix for item in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein = \"p38\"\n",
    "bench_folder = f\"/home/anna/Documents/benchmark\"\n",
    "main_dir = f\"{bench_folder}/reruns/{protein}\"\n",
    "engine = \"GROMACS\"\n",
    "\n",
    "ana_prot = analysis_protocol(\n",
    "    {\n",
    "        \"estimator\": \"MBAR\",\n",
    "        \"method\": \"alchemlyb\",\n",
    "        \"check overlap\": True,\n",
    "        \"try pickle\": True,\n",
    "        \"save pickle\": True,\n",
    "        \"auto equilibration\": False,\n",
    "        \"statistical inefficiency\": False,\n",
    "        \"truncate lower\": 0,\n",
    "        \"truncate upper\": 100,\n",
    "        \"name\": None,\n",
    "    }\n",
    ")\n",
    "\n",
    "net_file = f\"{main_dir}/execution_model/network_combined.dat\"\n",
    "exp_file = f\"{bench_folder}/inputs/experimental/{protein}.yml\"\n",
    "output_folder = f\"{main_dir}/outputs_extracted\"\n",
    "\n",
    "pipeline_prot = pipeline_protocol(\n",
    "    f\"{main_dir}/execution_model/protocol.dat\", auto_validate=True\n",
    ")  # no protocol used , name added after if needed\n",
    "# pipeline_prot = pipeline_protocol(auto_validate=True)\n",
    "\n",
    "# initialise the network object\n",
    "ana_obj = analysis_network(\n",
    "    output_folder,\n",
    "    exp_file=exp_file,\n",
    "    net_file=net_file,\n",
    "    analysis_prot=ana_prot,\n",
    "    # method=pipeline_prot.name(),  # if the protocol had a name\n",
    "    engines=[engine],\n",
    ")\n",
    "\n",
    "# compute\n",
    "ana_obj.compute_results()\n",
    "\n",
    "# # add ligands folder\n",
    "if os.path.isdir(f\"{bench_folder}/inputs/reruns/{protein}/ligands_intermediates\"):\n",
    "    ana_obj.add_ligands_folder(\n",
    "        f\"{bench_folder}/inputs/reruns/{protein}/ligands_intermediates\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ana_obj.method)\n",
    "perts = ana_obj.failed_perturbations(\"AMBER\")\n",
    "print(len(perts))\n",
    "with open(net_file.replace(\".dat\", \"_reverse.dat\"), \"w\") as f:\n",
    "    for pert in perts:\n",
    "        print(pert)\n",
    "        string = f\"{pert.split('~')[1]} {pert.split('~')[0]} 12 0.0000,0.0909,0.1818,0.2727,0.3636,0.4546,0.5454,0.6364,0.7273,0.8182,0.9091,1.0000 {engine}\\n\"\n",
    "        f.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ana_obj.method)\n",
    "perts = ana_obj.failed_perturbations(\"AMBER\")\n",
    "print(len(perts))\n",
    "with open(net_file.replace(\".dat\", \"_2fs.dat\"), \"w\") as f:\n",
    "    for pert in perts:\n",
    "        print(pert)\n",
    "        string = f\"{pert.split('~')[0]} {pert.split('~')[1]} 12 0.0000,0.0909,0.1818,0.2727,0.3636,0.4546,0.5454,0.6364,0.7273,0.8182,0.9091,1.0000 {engine}\\n\"\n",
    "        f.write(string)\n",
    "\n",
    "# with open(net_file.replace(\".dat\", \"_4fs.dat\"), \"w\") as f:\n",
    "#     for pert in ana_obj.perturbations:\n",
    "#         if pert not in perts:\n",
    "#             # print(pert)\n",
    "#             string = f\"{pert.split('~')[0]} {pert.split('~')[1]} 12 0.0000,0.0909,0.1818,0.2727,0.3636,0.4546,0.5454,0.6364,0.7273,0.8182,0.9091,1.0000 {engine}\\n\"\n",
    "#             f.write(string)\n",
    "\n",
    "# ana_obj.calc_rmse_engines(\"pert\", recalculate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite calc pert dict\n",
    "for prot in ana_obj_dict:\n",
    "    ana_obj = network_dict[\"lomap\"][prot][\"subsampling\"]\n",
    "\n",
    "    calc_pert_dict = {}\n",
    "    for eng in ana_obj.engines:\n",
    "        calc_pert_dict[eng] = {}\n",
    "        for pert in ana_obj._perturbations_dict[eng]:\n",
    "            vals_list = []\n",
    "            bound_vals_ = []\n",
    "            free_vals_ = []\n",
    "            try:\n",
    "                for r in [0, 1, 2]:\n",
    "                    bound_vals_.append(ana_obj.calc_repeat_bound_dict[eng][r][pert][0])\n",
    "                    free_vals_.append(ana_obj.calc_repeat_free_dict[eng][r][pert][0])\n",
    "                bound_vals = [x for x in bound_vals_ if str(x) != \"nan\"]\n",
    "                free_vals = [x for x in free_vals_ if str(x) != \"nan\"]\n",
    "                # print(bound_vals, free_vals)\n",
    "                if len(bound_vals) > 2:\n",
    "                    max_bound = (\n",
    "                        [abs(val) for val in bound_vals / np.mean(bound_vals)]\n",
    "                    ).index(max([abs(val) for val in bound_vals / np.mean(bound_vals)]))\n",
    "                    del bound_vals[max_bound]\n",
    "                if len(free_vals) > 2:\n",
    "                    max_free = (\n",
    "                        [abs(val) for val in free_vals / np.mean(free_vals)]\n",
    "                    ).index(max([abs(val) for val in free_vals / np.mean(free_vals)]))\n",
    "                    del free_vals[max_free]\n",
    "                free_avg = np.mean(free_vals)\n",
    "                bound_avg = np.mean(bound_vals)\n",
    "                if len(free_vals) == 1:\n",
    "                    free_err = ana_obj.calc_repeat_free_dict[eng][r][pert][1]\n",
    "                else:\n",
    "                    free_err = sem(free_vals)\n",
    "                if len(bound_vals) == 1:\n",
    "                    bound_err = ana_obj.calc_repeat_bound_dict[eng][r][pert][1]\n",
    "                else:\n",
    "                    bound_err = sem(bound_vals)\n",
    "                freenrg_val = bound_avg - free_avg\n",
    "                freenrg_err = math.sqrt(math.pow(bound_err, 2) + math.pow(free_err, 2))\n",
    "                freenrg_rel = (freenrg_val, freenrg_err)\n",
    "            except Exception as e:\n",
    "                freenrg_rel = (np.nan, np.nan)\n",
    "                print(e)\n",
    "                print(f\"{prot} {eng} {pert} does not have a value\")\n",
    "\n",
    "            calc_pert_dict[eng][pert] = freenrg_rel\n",
    "\n",
    "    ana_obj.calc_pert_dict = calc_pert_dict\n",
    "    # ana_obj._initialise_stats_object(check=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any pert greater than a certain value\n",
    "for network in network_dict:\n",
    "    ana_obj_dict = network_dict[network]\n",
    "    for prot in ana_obj_dict:\n",
    "        for name in ana_dicts:\n",
    "            print(prot, name)\n",
    "            ana_obj = ana_obj_dict[prot][name]\n",
    "\n",
    "            try:\n",
    "                for eng in ana_obj.engines:\n",
    "                    perts = []\n",
    "                    for pert in ana_obj.calc_pert_dict[eng]:\n",
    "                        if abs(ana_obj.calc_pert_dict[eng][pert][0]) > 10:\n",
    "                            perts.append(pert)\n",
    "                    ana_obj.remove_perturbations(perts, name=eng)\n",
    "                    print(f\"{perts} removed for {network}, {prot}, {name}, {eng}\")\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any pert with error greater than a certain value\n",
    "# for network in network_dict:\n",
    "network = \"lomap\"\n",
    "ana_obj_dict = network_dict[network]\n",
    "for prot in ana_obj_dict:\n",
    "    for name in ana_dicts:\n",
    "        print(prot, name)\n",
    "        ana_obj = ana_obj_dict[prot][name]\n",
    "\n",
    "        try:\n",
    "            for eng in ana_obj.engines:\n",
    "                perts = []\n",
    "                for pert in ana_obj.calc_pert_dict[eng]:\n",
    "                    if abs(ana_obj.calc_pert_dict[eng][pert][1]) > 1:\n",
    "                        perts.append(pert)\n",
    "                ana_obj.remove_perturbations(perts, name=eng)\n",
    "                print(f\"{perts} removed for {network}, {prot}, {name}, {eng}\")\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # consensus scoring 2\n",
    "# # picking the most similar from a euclidian similarity matrix\n",
    "\n",
    "# from scipy.spatial.distance import euclidean, pdist, squareform\n",
    "\n",
    "\n",
    "# def similarity_func(u, v):\n",
    "#     return 1 / (1 + euclidean(u, v))\n",
    "\n",
    "\n",
    "# for prot in ana_obj_dict.keys():\n",
    "#     print(prot)\n",
    "#     ana_obj = network_dict[\"lomap\"][prot][\"plain\"]\n",
    "#     pipeline.utils.validate.folder_path(\n",
    "#         f\"{ana_obj.files_folder}/consensus2\", create=True\n",
    "#     )\n",
    "\n",
    "#     pert_dict = {}\n",
    "#     for eng in ana_obj.engines:\n",
    "#         pert_dict[eng] = []\n",
    "\n",
    "#     for pert in ana_obj.perturbations:\n",
    "#         pert_list = []\n",
    "#         eng_list = []\n",
    "#         for eng in ana_obj.engines:\n",
    "#             try:\n",
    "#                 pert_list.append(ana_obj.calc_pert_dict[eng][pert][0])\n",
    "#                 eng_list.append(eng)\n",
    "#             except:\n",
    "#                 pass\n",
    "\n",
    "#         DF_var = pd.DataFrame.from_dict({pert: pert_list})\n",
    "#         DF_var.index = eng_list\n",
    "#         if len(eng_list) < 3:\n",
    "#             for eng in DF_var.index:\n",
    "#                 pert_dict[eng].append(pert)\n",
    "\n",
    "#         else:\n",
    "#             try:\n",
    "#                 dists = pdist(DF_var, similarity_func)\n",
    "#                 DF_euclid = squareform(dists)\n",
    "#                 # 1,2 is SOMD,GROMACS\n",
    "#                 # 0,1 is AMBER,SOMD\n",
    "#                 # 0,2 is AMBER, GROMACS\n",
    "#                 # pick the most similar\n",
    "#                 max_dict = {\n",
    "#                     DF_euclid[1][2]: [\"SOMD\", \"GROMACS\"],\n",
    "#                     DF_euclid[0][1]: [\"AMBER\", \"SOMD\"],\n",
    "#                     DF_euclid[0][2]: [\"AMBER\", \"GROMACS\"],\n",
    "#                 }\n",
    "#                 maximum_sim = max(max_dict.keys())\n",
    "#                 for eng in max_dict[maximum_sim]:\n",
    "#                     pert_dict[eng].append(pert)\n",
    "#             except:\n",
    "#                 # if val is missing, use the other two engines\n",
    "#                 DF_var.dropna(inplace=True)\n",
    "#                 for eng in DF_var.index:\n",
    "#                     pert_dict[eng].append(pert)\n",
    "\n",
    "#     mod_results_files_list = []\n",
    "\n",
    "#     for eng in ana_obj.engines:\n",
    "#         mod_results_files = write_modified_results_files(\n",
    "#             results_files=ana_obj._results_repeat_files[eng],\n",
    "#             perturbations=pert_dict[eng],\n",
    "#             name=\"consensus2\",\n",
    "#             output_folder=f\"{ana_obj.files_folder}/consensus2\",\n",
    "#         )\n",
    "#         mod_results_files_list.append(mod_results_files)\n",
    "\n",
    "#     mod_results_files_list = flatten_comprehension(mod_results_files_list)\n",
    "\n",
    "#     try:\n",
    "#         ana_obj.compute_other_results(mod_results_files_list, name=\"consensus2\")\n",
    "#     except:\n",
    "#         print(f\"could not for {prot}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipeline_annamherz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

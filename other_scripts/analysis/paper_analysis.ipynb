{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reproducibility between perturbations\n",
    "\n",
    "this is for the combined networks\n",
    "\n",
    "check:\n",
    "vs experimental\n",
    "vs each other\n",
    "outliers for each engine\n",
    "most different perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_obj = ana_obj_dict[\"combined\"]\n",
    "\n",
    "# compared to each other\n",
    "mad_df, mad_df_err = ana_obj.calc_mad_engines(pert_val=\"pert\")\n",
    "print(mad_df)\n",
    "print(mad_df_err)\n",
    "\n",
    "# # compared to experimental\n",
    "mae_df, mae_df_err = ana_obj.calc_mae_engines(pert_val=\"pert\")\n",
    "print(mae_df)\n",
    "print(mae_df_err)\n",
    "\n",
    "# also saved in output folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking results, convergence, spread of data between engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ana_obj in ana_obj_dict.values():\n",
    "    # can compute convergence for all\n",
    "    ana_obj.compute_convergence(main_dir=main_dir)\n",
    "    ana_obj.plot_convergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms\n",
    "\n",
    "for ana_obj in ana_obj_dict.values():\n",
    "    ana_obj.plot_histogram_repeats()\n",
    "\n",
    "    ana_obj.plot_histogram_legs()\n",
    "\n",
    "    ana_obj.plot_histogram_sem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outliers for each engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_obj.plot_outliers(no_outliers=5, engines=ana_obj.engines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eng in ana_obj.engines:\n",
    "    # get outliers above a certain threshold\n",
    "    perts = ana_obj.get_outliers(threshold=3, name=eng)\n",
    "    print(f\"{eng} : {perts}\")\n",
    "\n",
    "    # draw the perturbations\n",
    "    ana_obj.draw_perturbations(perts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perturbations that are the most different between the engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_engines_list_dict = {}\n",
    "\n",
    "for pert in ana_obj.perturbations:\n",
    "    all_engines_list_dict[pert] = []\n",
    "\n",
    "    for eng in ana_obj.engines:\n",
    "        all_engines_list_dict[pert].append(ana_obj.calc_pert_dict[eng][pert][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_largest_difference_pert(dictionary_of_lists, num_ranges):\n",
    "    ranges = {}\n",
    "\n",
    "    for key, sublist in dictionary_of_lists.items():\n",
    "        range = max(sublist) - min(sublist)\n",
    "        ranges[key] = range\n",
    "\n",
    "    sorted_ranges = sorted(ranges.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_ranges = sorted_ranges[:num_ranges]\n",
    "\n",
    "    return top_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ranges = find_top_largest_difference_pert(all_engines_list_dict, 5)\n",
    "print(top_ranges)\n",
    "perts = [a[0] for a in top_ranges]\n",
    "ana_obj.draw_perturbations(perts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_obj._plotting_object.scatter(\"pert\", values=perts, y_names=ana_obj.engines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eng in ana_obj.engines:\n",
    "    print(eng)\n",
    "    for key in ana_obj.calc_pert_dict[eng].items():\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cycle closures\n",
    "\n",
    "for each network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for net in ana_obj_dict:\n",
    "    print(net)\n",
    "    ana_obj = ana_obj_dict[net]\n",
    "\n",
    "    ana_obj.compute_cycle_closures()\n",
    "\n",
    "    for eng in ana_obj.cycle_dict:\n",
    "        print(eng)\n",
    "        cycles = ana_obj.cycle_dict[eng]\n",
    "        print(f\"{eng} cycle vals is {cycles[1]}\")\n",
    "        print(f\"{eng} cycle mean is {cycles[2]}\")\n",
    "        print(f\"{eng} cycle deviation is {cycles[3]}\")\n",
    "\n",
    "        max_cycle_ind = max(cycles[1])\n",
    "        max_cycle = list(cycles[0].keys())[cycles[1].index(max_cycle_ind)]\n",
    "        print(max_cycle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reproducibility between ligs - per ligand results\n",
    "\n",
    "this is for the individual networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for each network and calc mad / mae\n",
    "\n",
    "\n",
    "for net in ana_obj_dict:\n",
    "    print(net)\n",
    "    ana_obj = ana_obj_dict[net]\n",
    "\n",
    "    # plotting with r2, spearmans rank\n",
    "    # for eng in ana_obj.engines:\n",
    "        # title = \"\"\n",
    "        # title += f\"{net}\"\n",
    "\n",
    "        # mue_exp = r2val = stats[\"val\"][\"experimental\"][eng][\"MUE\"]\n",
    "        # r2val = stats[\"val\"][\"experimental\"][eng][\"R2\"]\n",
    "        # rmseval = stats[\"val\"][\"experimental\"][eng][\"RMSE\"]\n",
    "        # spearman = stats[\"val\"][\"experimental\"][eng][\"rho\"]\n",
    "\n",
    "        # # titles\n",
    "        # title += f\"\\n MAE : {mue_exp[0]:.2f} +/- {mue_exp[1]:.2f} kcal/mol\"\n",
    "        # title += f\"\\n R2 : {r2val[0]:.2f} +/- {r2val[1]:.2f} kcal/mol\"\n",
    "        # title += f\"\\n RMSE : {rmseval[0]:.2f} +/- {rmseval[1]:.2f} kcal/mol\"\n",
    "        # title += f\"\\n rho : {spearman[0]:.2f} +/- {spearman[1]:.2f} kcal/mol\"\n",
    "\n",
    "        # kwargs = {\"title\": title}\n",
    "        # ana_obj.plot_scatter_dG(engine=eng, **kwargs)\n",
    "        # ana_obj.plot_scatter_dG(engine=eng, use_cinnabar=True)\n",
    "\n",
    "    # compared to each other\n",
    "    print(\"mad\")\n",
    "    mad_df, mad_df_err = ana_obj.calc_mad_engines(pert_val=\"val\")\n",
    "    print(mad_df)\n",
    "    print(mad_df_err)\n",
    "\n",
    "    # # compared to experimental\n",
    "    print(\"mae\")\n",
    "    mae_df, mae_df_err = ana_obj.calc_mae_engines(pert_val=\"val\")\n",
    "    print(mae_df)\n",
    "    print(mae_df_err)\n",
    "\n",
    "    # # compared to experimental\n",
    "    print(\"ktau\")\n",
    "    mae_df, mae_df_err = ana_obj.calc_kendalls_rank_engines(pert_val=\"val\")\n",
    "    print(mae_df)\n",
    "    print(mae_df_err)\n",
    "\n",
    "    print(\"spearmans\")\n",
    "    mae_df, mae_df_err = ana_obj.calc_spearmans_rank_engines(pert_val=\"val\")\n",
    "    print(mae_df)\n",
    "    print(mae_df_err)\n",
    "\n",
    "    print(\"r2\")\n",
    "    mae_df, mae_df_err = ana_obj.calc_r2_engines(pert_val=\"val\")\n",
    "    print(mae_df)\n",
    "    print(mae_df_err)\n",
    "    # also saved in output folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### different network analysis methods\n",
    "\n",
    "all so far with cinnabar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compared to fwf\n",
    "\n",
    "for net in ana_obj_dict:\n",
    "    print(net)\n",
    "    ana_obj = ana_obj_dict[net]\n",
    "\n",
    "    # first need to add the fwf path\n",
    "    ana_obj._add_fwf_path(\n",
    "        \"/home/anna/Documents/september_2022_workshops/freenrgworkflows/networkanalysis\"\n",
    "    )\n",
    "\n",
    "    # all_analysis_object._add_fwf_path(\n",
    "    #     \"/home/anna/Documents/freenrgworkflows/networkanalysis\"\n",
    "    # )\n",
    "\n",
    "    title = \"\"\n",
    "    title += f\"{net}\"\n",
    "\n",
    "    for eng in ana_obj.engines:\n",
    "        print(eng)\n",
    "        # get the network analysis\n",
    "        fwf_dict = ana_obj._get_ana_fwf(engine=eng)\n",
    "        # for key in fwf_dict:\n",
    "        #     print(f\"{key} : {fwf_dict[key][0]}, {fwf_dict[key][1]}\")\n",
    "\n",
    "        # get fwf stats\n",
    "        r_confidence, tau_confidence, mue_confidence = ana_obj._get_stats_fwf(engine=eng)\n",
    "        print(\"r: \", r_confidence)\n",
    "        print(\"tau: \", tau_confidence)\n",
    "        print(\"mae: \", mue_confidence)\n",
    "\n",
    "    # compared to each other\n",
    "    mad_df, mad_df_err = ana_obj._get_mad_fwf(ana_obj.engines, ana_obj.engines)\n",
    "    print(mad_df)\n",
    "    print(mad_df_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compared to mbarnet\n",
    "\n",
    "# ana_obj_dict[\"combined\"].analyse_mbarnet(compute_missing=True, use_experimental=True, write_xml=True, run_xml_py=True)\n",
    "\n",
    "for net in ana_obj_dict:\n",
    "    print(net)\n",
    "    ana_obj = ana_obj_dict[net]\n",
    "\n",
    "    ana_obj.analyse_mbarnet(compute_missing=False, use_experimental=True, write_xml=False, run_xml_py=False)\n",
    "\n",
    "    statistics = [\"MUE\", \"R2\", \"rho\", \"KTAU\"]\n",
    "\n",
    "    for stats in statistics:\n",
    "        print(stats)\n",
    "        df,dferr = ana_obj._get_stats_mbarnet(statistic=stats)\n",
    "        print(df)\n",
    "        print(dferr)\n",
    "\n",
    "    # compared to each other\n",
    "    mad_df, mad_df_err = ana_obj._get_mad_mbarnet(ana_obj.engines, ana_obj.engines)\n",
    "    print(mad_df)\n",
    "    print(mad_df_err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting just fwf data per ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_y = fwf_dict\n",
    "dict_exp = exp_dicts[0]\n",
    "\n",
    "df1 = plotting_engines.match_dicts_to_df(dict_exp, dict_y, \"experimental\", \"fwf\")\n",
    "df1\n",
    "\n",
    "df1.plot.bar(\n",
    "    y=[\"freenrg_fwf\", \"freenrg_experimental\"],\n",
    "    yerr=df1[[\"err_fwf\", \"err_experimental\"]].T.values,\n",
    "    title=f\"fwf, experimental, {eng}\",\n",
    "    xlabel=\"ligands\",\n",
    "    ylabel=\"dG (kcal/mol)\",\n",
    ")\n",
    "\n",
    "df1.dropna()\n",
    "df1.plot.scatter(\n",
    "    x=\"freenrg_experimental\",\n",
    "    y=\"freenrg_fwf\",\n",
    "    xerr=\"err_experimental\",\n",
    "    yerr=\"err_fwf\",\n",
    "    title=f\"fwf, {eng}\",\n",
    "    xlabel=\"experimental dG (kcal/mol)\",\n",
    "    ylabel=\"fwf dG (kcal/mol)\",\n",
    ")\n",
    "\n",
    "# calculating using the cinnabar stats\n",
    "f_mae = all_analysis_object._stats_object._compute_stats(\n",
    "    x=df1[\"freenrg_experimental\"],\n",
    "    y=df1[\"freenrg_fwf\"],\n",
    "    xerr=df1[\"err_experimental\"],\n",
    "    yerr=df1[\"err_fwf\"],\n",
    "    statistic=\"MUE\",\n",
    ")\n",
    "print(f_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### consensus\n",
    "\n",
    "consensus scoring of the engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average of averages\n",
    "\n",
    "for net in ana_obj_dict:\n",
    "    ana_obj = ana_obj_dict[net]\n",
    "\n",
    "    consensus_pert_dict = {}\n",
    "\n",
    "    ana_obj.compute_consensus()\n",
    "\n",
    "    print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consensus scoring, is it more robust\n",
    "\n",
    "for net in ana_obj_dict:\n",
    "    print(net)\n",
    "\n",
    "    ana_obj = ana_obj_dict[net]\n",
    "\n",
    "    for pv in [\"pert\", \"val\"]:\n",
    "        print(pv)\n",
    "        mae_df, mae_df_err = ana_obj.calc_mae_engines(pv, engines=\"consensus\")\n",
    "        print(mae_df)\n",
    "        print(mae_df_err)\n",
    "\n",
    "        stat_rank = ana_obj._stats_object.compute_rho(pv, y=\"consensus\")\n",
    "        print(stat_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### directionality\n",
    "\n",
    "data from featurising the perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_obj = ana_obj_dict[\"combined\"]\n",
    "\n",
    "grow_shrink_dict = {}\n",
    "\n",
    "for eng in ana_obj.engines:\n",
    "    grow_shrink_dict[eng] = {}\n",
    "\n",
    "    error_dict = {\n",
    "        key: ana_obj.calc_pert_dict[eng][key][1] for key in ana_obj.calc_pert_dict[eng]\n",
    "    }\n",
    "    df = pd.read_csv(f\"{main_dir}/execution_model/grow_shrink_featurise.dat\")\n",
    "    df[f\"error_{eng}\"] = df[\"pert\"].map(error_dict)\n",
    "    df = df.dropna()\n",
    "\n",
    "    group1 = df.loc[df[\"grow/shrink\"] == \"grow\"][f\"error_{eng}\"]\n",
    "    group2 = df.loc[df[\"grow/shrink\"] == \"shrink\"][f\"error_{eng}\"]\n",
    "    ustats, pvalue = scipy.stats.mannwhitneyu(group1, group2)\n",
    "    print(f\"mann u for error {eng}: {ustats, pvalue}\")\n",
    "    print(\n",
    "        f\"mean for error {eng} grow: {np.mean(group1)}, and for shrink: {np.mean(group2)}\"\n",
    "    )\n",
    "\n",
    "    grow_shrink_dict[eng][\"grow_err\"] = group1\n",
    "    grow_shrink_dict[eng][\"shrink_err\"] = group2\n",
    "\n",
    "    # for diff to experimental\n",
    "    diff_dict = {\n",
    "        key: diff_to_exp_dict[\"combined\"][eng][key]\n",
    "        for key in ana_obj.calc_pert_dict[eng]\n",
    "    }\n",
    "    df[f\"diff_{eng}\"] = df[\"pert\"].map(diff_dict)\n",
    "    df = df.dropna()\n",
    "\n",
    "    group1 = df.loc[df[\"grow/shrink\"] == \"grow\"][f\"diff_{eng}\"]\n",
    "    group2 = df.loc[df[\"grow/shrink\"] == \"shrink\"][f\"diff_{eng}\"]\n",
    "    ustats, pvalue = scipy.stats.mannwhitneyu(group1, group2)\n",
    "    print(f\"mann u for diff to exp {eng}: {ustats, pvalue}\")\n",
    "    print(\n",
    "        f\"mean for diff to exp {eng} grow: {np.mean(group1)}, and for shrink: {np.mean(group2)}\"\n",
    "    )\n",
    "\n",
    "    grow_shrink_dict[eng][\"grow_diff\"] = group1\n",
    "    grow_shrink_dict[eng][\"shrink_diff\"] = group2\n",
    "\n",
    "# if below 0.05 (if confidence interval) there is significant difference (reject null hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different between engines significant?\n",
    "\n",
    "res_dict = {}\n",
    "\n",
    "for size in [\"grow_err\", \"shrink_err\", \"grow_diff\", \"shrink_diff\"]:\n",
    "    res_dict[size] = {}\n",
    "\n",
    "    for eng in ana_obj_dict[\"combined\"].engines:\n",
    "        res_dict[size][eng] = {}\n",
    "\n",
    "    for combo in it.product(grow_shrink_dict.keys(), grow_shrink_dict.keys()):\n",
    "        eng1 = combo[0]\n",
    "        eng2 = combo[1]\n",
    "\n",
    "        if eng1 == eng2:\n",
    "            continue\n",
    "\n",
    "        group1 = grow_shrink_dict[eng1][size]\n",
    "        group2 = grow_shrink_dict[eng2][size]\n",
    "\n",
    "        ustats, pvalue = scipy.stats.mannwhitneyu(group1, group2)\n",
    "        print(f\"{eng1, eng2}, {size}: {ustats, pvalue}\")\n",
    "        print(f\"mean for {eng1}: {np.mean(group1)}, and for {eng2}: {np.mean(group2)}\")\n",
    "\n",
    "        res_dict[size][eng1][eng2] = pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(res_dict[\"grow_err\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "size of perturbation and variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_obj = ana_obj_dict[\"combined\"]\n",
    "\n",
    "file = f\"{bench_folder}/extracted/{protein}/perturbing_overlap.dat\"\n",
    "\n",
    "for eng in ana_obj.engines:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    df = df[df[\"engine\"] == eng]\n",
    "\n",
    "    error_dict = {\n",
    "        key: ana_obj.calc_pert_dict[eng][key][1] for key in ana_obj.calc_pert_dict[eng]\n",
    "    }\n",
    "    df[f\"error\"] = df[\"perturbation\"].map(error_dict)\n",
    "    # df = df.dropna()\n",
    "    df = df[df[\"error\"].notna()]\n",
    "    df = df[df[\"perturbing_atoms\"].notna()]\n",
    "\n",
    "    stats = pipeline.analysis.stats_engines.compute_stats(\n",
    "        [x for x in df[\"perturbing_atoms\"]], [x for x in df[\"error\"]], statistic=\"R2\"\n",
    "    )\n",
    "    df.plot.scatter(\n",
    "        \"perturbing_atoms\",\n",
    "        \"error\",\n",
    "        c=\"diff_to_exp\",\n",
    "        colormap=\"viridis\",\n",
    "        title=f\"{eng}\\n{stats}\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
